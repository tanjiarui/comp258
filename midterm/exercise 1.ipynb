{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJX3ezb-ZS9j"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrtJpk_N2_f4"
      },
      "source": [
        "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
        "def backprop(W1, W2, X, Y, alpha=.2):\n",
        "\tfor b in range(X.shape[0]):\n",
        "\t\tx = X[b, :].T  # inputs from training data\n",
        "\t\ty = Y[b]  # correct output from training data\n",
        "\t\t##########################\n",
        "\t\t# forward propagation step\n",
        "\t\t##########################\n",
        "\t\t# calculate the weighted sum of hidden node\n",
        "\t\tv1 = np.dot(W1, x)\n",
        "\t\t# pass the weighted sum to the activation function, this gives the outputs from hidden layer\n",
        "\t\ty1 = sigmoid(v1)\n",
        "\t\t# calculate the weighted sum of the output layer\n",
        "\t\tv = np.dot(W2, y1)\n",
        "\t\t# pass it to the activation function, this returns the output of the third layer\n",
        "\t\ty_hat = sigmoid(v)\n",
        "\t\t# calculate the error, difference between correct output and computed output\n",
        "\t\terror = y - y_hat\n",
        "\t\t# calculate delta, derivative of the activation function times the error\n",
        "\t\t# note that ùúé‚Ä≤(ùë•)=ùúé(ùë•)‚àô(1‚àí ùúé(ùë•)) = y * (1-y)\n",
        "\t\tdelta = y_hat * (1 - y_hat) * error  # element wise multiplication\n",
        "\t\t###########################\n",
        "\t\t# Backward propagation step\n",
        "\t\t# Stochastic Gradient Descent\n",
        "\t\t###########################\n",
        "\t\t# propagate the output node delta, Œ¥, backward, and calculate the deltas of the hidden layer.\n",
        "\t\te1 = np.dot(W2.T, delta)\n",
        "\t\tdelta1 = y1 * (1 - y1) * e1  # element wise multiplication\n",
        "\n",
        "\t\t# Adjust the weights according to the learning rule\n",
        "\t\tdelta1 = delta1.reshape(delta1.size, 1)\n",
        "\t\tx = x.reshape(1, x.size)\n",
        "\t\tdW1 = alpha * np.dot(delta1, x)\n",
        "\t\tW1 = W1 + dW1\n",
        "\t\tdelta = delta.reshape(delta.size, 1)\n",
        "\t\ty1 = y1.reshape(1, y1.size)\n",
        "\t\tdW2 = alpha * np.dot(delta, y1)\n",
        "\t\tW2 = W2 + dW2\n",
        "\n",
        "\treturn W1, W2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbUG0W_O3B_r"
      },
      "source": [
        "def train(X, y, node, lr=.2, epoch=1000):\n",
        "\tinput_size, output_size = X.shape[1], np.unique(y).size\n",
        "\t# initialize the weights between input layer and hidden layer\n",
        "\tW1 = 2 * np.random.rand(node, input_size) - 1\n",
        "\t# initialize the weights between hidden layer and output layer\n",
        "\tW2 = 2 * np.random.rand(output_size, node) - 1\n",
        "\tfor e in range(epoch): # train\n",
        "\t\tW1, W2 = backprop(W1, W2, X, y, lr)\n",
        "\treturn W1, W2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P7brljx3Y0a"
      },
      "source": [
        "def predict(X, W1, W2):\n",
        "\ty_prob, y = [], []\n",
        "\tfor b in range(X.shape[0]):\n",
        "\t\tx = X[b, :].T\n",
        "\t\t# calculate the weighted sum of hidden node\n",
        "\t\tv1 = np.dot(W1, x)\n",
        "\t\t# pass the weighted sum to the activation function, this gives the outputs from hidden layer\n",
        "\t\ty1 = sigmoid(v1)\n",
        "\t\t# calculate the weighted sum of the output layer\n",
        "\t\tv = np.dot(W2, y1)\n",
        "\t\t# pass it to the activation function, this returns the output of the third layer\n",
        "\t\ty_prob.append(max(sigmoid(v)))\n",
        "\t\ty.append(np.argmax(sigmoid(v)))\n",
        "\treturn y_prob, y"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwJj9fme38A3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "379684de-df85-482c-a80f-2c508d31dab4"
      },
      "source": [
        "data = np.loadtxt('banknote authentication.csv', delimiter=',')\n",
        "x_train, x_test, y_train, y_test = train_test_split(data[:, :4], data[:, 4].astype(int), test_size=.2)\n",
        "for alpha in np.arange(.01, .1, .02):\n",
        "\tprint('learning rate: ' + str(alpha))\n",
        "\tw1, w2 = train(x_train, y_train, 6, alpha)\n",
        "\tpredict_prob, prediction = predict(x_test, w1, w2)\n",
        "\tprint('test set')\n",
        "\tprint(x_test)\n",
        "\tprint('label')\n",
        "\tprint(y_test)\n",
        "\tprint('prediction')\n",
        "\tprint(predict_prob)\n",
        "\tprint(classification_report(y_test, prediction))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning rate: 0.01\n",
            "test set\n",
            "[[ -1.8391   -9.0883    9.2416   -0.10432]\n",
            " [ -3.6961  -13.6779   17.5795   -2.6181 ]\n",
            " [ -4.9462    3.5716    0.82742  -1.4957 ]\n",
            " ...\n",
            " [  0.47368   3.3605   -4.5064   -4.0431 ]\n",
            " [ -1.2576    1.5892    7.0078    0.42455]\n",
            " [  0.59823   3.5012   -3.9795   -1.7841 ]]\n",
            "label\n",
            "[1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 0 1\n",
            " 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1\n",
            " 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0\n",
            " 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1]\n",
            "prediction\n",
            "[0.9947542007882139, 0.9886936790292583, 0.9991676459351756, 0.9959581626057405, 0.9953961211495185, 0.9999937591836382, 0.9867062710221725, 0.00875946714907824, 0.008399475020675137, 0.008802518796848135, 0.999823547824645, 0.9990769265171527, 0.9971977090500832, 0.9999444132618006, 0.008564648830070382, 0.9955476325339173, 0.008354421958665828, 0.9996467462569979, 0.008404604087482052, 0.0197823517009115, 0.00843406852211251, 0.015780581719753006, 0.05873154718630064, 0.009629368759339915, 0.9916863876547968, 0.9508530977598063, 0.008431999267167013, 0.9854651021021473, 0.995288342571506, 0.008597890856821973, 0.9977228005550337, 0.9999167588687313, 0.9981717490349009, 0.00909185451584848, 0.008371883332251303, 0.9688373451869724, 0.008350191281061669, 0.02051910642889779, 0.00837549503500556, 0.009058403117893382, 0.008541308662733959, 0.9944984192530761, 0.010599183927927186, 0.015280406672802456, 0.9951714663331164, 0.990591695563669, 0.008861444708105003, 0.008760789101195383, 0.996940331340888, 0.99450012970052, 0.00849693859809758, 0.008505045514360926, 0.008386497995742275, 0.9997750208019592, 0.009289964743106578, 0.9997389834668209, 0.9992360956048313, 0.008708835985559781, 0.9949917888758862, 0.9999816618626669, 0.9975998125909312, 0.27364347632962405, 0.9962296244542275, 0.9999677330453344, 0.08708791069022494, 0.010416822583564595, 0.9978829848662282, 0.9999704219344238, 0.9583377155414621, 0.013284822826406585, 0.9977048771145216, 0.008774134995804575, 0.008677599955434113, 0.9999517036733816, 0.9668851175007925, 0.008664324512106985, 0.009028676340936551, 0.015630694950818844, 0.9990769279017544, 0.008514187316905583, 0.9989219844321999, 0.0085116968216565, 0.009588263523823927, 0.008537741286706296, 0.009038509191132173, 0.9720756772784893, 0.008619649882828323, 0.008843018115788673, 0.010735115571591935, 0.9965281434281491, 0.9704382770521928, 0.9999952845271067, 0.9983831722529826, 0.9976116207698928, 0.9966343432619725, 0.9999066603835124, 0.9959571838537575, 0.9360925353090477, 0.9949120450742098, 0.9668915757241565, 0.9982055898257872, 0.9999879220869601, 0.9971906137868053, 0.9999835752705577, 0.9969877557816457, 0.00900505441668754, 0.9984470246354502, 0.009287876343644814, 0.9990063800622648, 0.008418900573599194, 0.999967009767196, 0.9999496960189359, 0.9864268736802047, 0.00976011380013796, 0.9964183427597799, 0.00864683783576519, 0.019611763961200727, 0.013003181214793456, 0.9999805720921768, 0.9964541701669561, 0.9964921337043459, 0.008490695138495574, 0.008962390750837988, 0.008432963186415695, 0.011010253596262957, 0.00861905214512939, 0.016414913691821426, 0.9995958097098597, 0.008616938708026888, 0.9851957857148056, 0.9961056435257551, 0.00873047516234054, 0.9971672874931669, 0.008994423571635487, 0.009731267063456908, 0.9993967161030676, 0.07099684296295072, 0.011248039481564122, 0.008394740533075531, 0.9947888137932457, 0.008368261490766842, 0.9897272909223205, 0.008888479605898997, 0.9765740745598314, 0.9898935226203709, 0.9992388483980352, 0.009485638528525358, 0.00836327718145184, 0.008347909385057925, 0.010376430039161635, 0.008402304644723716, 0.00841456366207612, 0.9483859727578963, 0.008515794664443585, 0.008860195983693418, 0.012119859850277529, 0.9973354267927828, 0.008630618646587644, 0.009148520826814756, 0.9997575251647376, 0.008442047739144646, 0.9979665165678581, 0.9968451108256061, 0.008416714677796325, 0.008825966439022365, 0.9697255992526104, 0.9974038331061735, 0.9995442683094443, 0.9924129408925357, 0.9986601768561117, 0.99884448058387, 0.008471401090813293, 0.999936611824145, 0.9990607513148829, 0.9900184667184949, 0.01004266110696758, 0.9968647454915182, 0.9998366282525523, 0.008362575480178016, 0.008606635147962383, 0.010544340614492746, 0.9999407263278667, 0.9991046025037236, 0.9998815472727409, 0.9980352488244656, 0.012368003978347359, 0.00861687002095473, 0.9964960892506931, 0.9986109596302151, 0.9999811930627864, 0.008447385719989221, 0.9997027678475239, 0.008475993274573523, 0.9974943955940165, 0.008389580167826746, 0.00892858816343203, 0.8862368402599877, 0.9780070503016645, 0.9980500973338982, 0.008475769354679011, 0.9994965539332502, 0.008414364173095913, 0.9999585706883181, 0.008388235233083104, 0.009366014007477223, 0.0104221479586181, 0.9999939855563669, 0.009889959587964224, 0.009280688344247149, 0.010495779524323784, 0.9997731629633146, 0.9909320180613124, 0.9987656975653554, 0.9999856303785747, 0.010810941925937503, 0.9999939268985422, 0.9931093741021869, 0.9564404983872401, 0.009913183472209031, 0.008353531136744587, 0.99912138232509, 0.018644836653084862, 0.008383145204601056, 0.0083765972430354, 0.008455016916972714, 0.009026293004431703, 0.008858589236455845, 0.9943386471351937, 0.008793804169784628, 0.00834999731764875, 0.008522670563415288, 0.9996106089637204, 0.008561657892923364, 0.017393301524976975, 0.9969711194826393, 0.9984895610420198, 0.9989658287841962, 0.009305102996388188, 0.008634099088859088, 0.9970062518482194, 0.9955687796965296, 0.008663660634515154, 0.9984868028176086, 0.9999617858001473, 0.021312361854083816, 0.9977974810807018, 0.01009636908605715, 0.008419713635154072, 0.008437442928530018, 0.9994417698906075, 0.011536773980835737, 0.011518502542357792, 0.08937564175674374, 0.008429839746267489, 0.008410778627314456, 0.008744943915245423, 0.9985523566497533, 0.9953602123138089, 0.008639484778506549, 0.008780642102483083, 0.008409007913551198, 0.008379349488208565, 0.008973637029681392, 0.008459043832871075, 0.008419361203179819, 0.9999741562141881, 0.9507613630536664, 0.009421648833387659, 0.008980059222019854, 0.010185866253520756, 0.08937564175674374, 0.009358149024497664, 0.9985318297601949, 0.010755516132153226, 0.9959476107782674]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.96      0.84       141\n",
            "           1       0.94      0.66      0.78       134\n",
            "\n",
            "    accuracy                           0.81       275\n",
            "   macro avg       0.84      0.81      0.81       275\n",
            "weighted avg       0.84      0.81      0.81       275\n",
            "\n",
            "learning rate: 0.03\n",
            "test set\n",
            "[[ -1.8391   -9.0883    9.2416   -0.10432]\n",
            " [ -3.6961  -13.6779   17.5795   -2.6181 ]\n",
            " [ -4.9462    3.5716    0.82742  -1.4957 ]\n",
            " ...\n",
            " [  0.47368   3.3605   -4.5064   -4.0431 ]\n",
            " [ -1.2576    1.5892    7.0078    0.42455]\n",
            " [  0.59823   3.5012   -3.9795   -1.7841 ]]\n",
            "label\n",
            "[1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 0 1\n",
            " 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1\n",
            " 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0\n",
            " 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1]\n",
            "prediction\n",
            "[0.9982897218332757, 0.9912202470539156, 0.9993263355085328, 0.997057717598743, 0.9986805439381261, 0.9999994263867622, 0.9844432835729906, 0.0037540793532974586, 0.0036489025060585335, 0.0037796647945163614, 0.9999746391401751, 0.9992265842212673, 0.9992447427076868, 0.9999770631231849, 0.0036944490082211697, 0.9980570616620006, 0.003631268079446761, 0.9999519160199551, 0.0037237777636767435, 0.007303697448996525, 0.0038283168709991166, 0.006125602630594473, 0.02583822908964664, 0.004231991364187516, 0.9967365593977106, 0.9596167505510278, 0.004009622573823859, 0.9937866913791457, 0.9986672535885056, 0.003807049414622309, 0.9980537816088616, 0.9999416053811154, 0.9958649911594033, 0.004120993667472396, 0.0036267128674588875, 0.9895236670900898, 0.0036616510704493466, 0.007816762663114298, 0.003635927634363575, 0.0038796999232170123, 0.0036697097023605193, 0.9979019811498151, 0.004541348801481644, 0.010959178452831545, 0.9959913802794507, 0.9970148011773131, 0.004049676527057913, 0.00722846013393263, 0.9988803591756025, 0.9975299947657078, 0.004937276191235254, 0.003668115823430728, 0.0036991984431105586, 0.9998293319707809, 0.004278512721566809, 0.999813406391097, 0.9990243047426018, 0.0038310771479398136, 0.9982364005918182, 0.9999821605241148, 0.9990510661394838, 0.039535534352275525, 0.9988501054376936, 0.9999972858084915, 0.05052343865224187, 0.004296270158664362, 0.9992485735725377, 0.9999968634953584, 0.9782296756713033, 0.005736750816689587, 0.999270749385838, 0.003820353717955317, 0.003793248635017793, 0.9999798408913014, 0.9786375520573755, 0.004048713610355851, 0.00416204017461943, 0.007676863988365096, 0.9998443998571931, 0.0036765449305401765, 0.9997156597639693, 0.0036724050776945814, 0.005953951556486318, 0.00403201199052996, 0.004056002488464691, 0.9854594639378533, 0.00423154270919516, 0.0038256092493085575, 0.0043864495219811395, 0.9988638006813839, 0.976882983407566, 0.9999995043932137, 0.9879759161693406, 0.9992207924755967, 0.9988165918379653, 0.9999641001630233, 0.9987702791086605, 0.9714551659409036, 0.9983297135405418, 0.97694284785583, 0.9995470422286086, 0.9999855058328142, 0.9990464118171363, 0.9999982571316064, 0.9982143241015736, 0.004569815594885692, 0.9995984011435518, 0.0039300540079612286, 0.9996999012620884, 0.0036599666692515424, 0.9999969618536368, 0.9999580714943356, 0.9975370968224883, 0.004061299610354385, 0.9988071003799708, 0.0037233576107180335, 0.008555101297069096, 0.011069252602028254, 0.9999809530210254, 0.9987546002232324, 0.9988702438731981, 0.0037071847228920315, 0.004008873891172274, 0.0036733527437379297, 0.005747852121794096, 0.003764569747471335, 0.0063727449720567255, 0.9998775416762303, 0.003762625746175821, 0.993558787786166, 0.9941263964158542, 0.0037719860414618576, 0.9989568619291863, 0.004132904359427514, 0.008335434888739907, 0.9998981399691481, 0.03772640086430715, 0.005273555265551151, 0.003634164218806775, 0.9966760521005084, 0.0036431294044129217, 0.9961652247897647, 0.004904049584922437, 0.9905424708104467, 0.9929146439681116, 0.9997681649470962, 0.004334473132457601, 0.0036758326615770245, 0.003639524354403624, 0.004249665367679523, 0.0036563249286197495, 0.0037429460076342106, 0.9547540983896959, 0.00388950787771093, 0.021515508037114998, 0.004937965806431556, 0.9990005104863556, 0.003951481132631925, 0.004527492024376801, 0.9999579046814109, 0.0037023846921633336, 0.9994093583654465, 0.9989089410279492, 0.003682502108584919, 0.0037955898168234143, 0.9800971960620465, 0.99903878186283, 0.999855389021646, 0.9961341576136545, 0.9996618765327836, 0.9997279607757198, 0.0036668778728393137, 0.9999905602661148, 0.9996947401193897, 0.9864147927779778, 0.004197400525246368, 0.9989442976601889, 0.9999439926814585, 0.0036451838488304905, 0.0037765789877975894, 0.006277585696594843, 0.9999540174366678, 0.9993698731613395, 0.9999809519372617, 0.9994010588242682, 0.00576575455093311, 0.0038224638978287206, 0.9989230707930156, 0.9977043808680289, 0.9999771269003963, 0.0037753746745160287, 0.9999170008347467, 0.003916602091270625, 0.9988941951193895, 0.0036876211130818398, 0.0038167711558528584, 0.9561544054493881, 0.9917761344951311, 0.9878286778234989, 0.0036978648068546204, 0.9998650566916943, 0.0036659888477274354, 0.9999958182398377, 0.003659213858007444, 0.004465688867488511, 0.0042802330750592405, 0.9999989316308772, 0.004449057279200114, 0.00412274379360474, 0.004471813439596256, 0.9999188728438193, 0.9989176819478203, 0.9997091213470342, 0.9999845159107762, 0.0054903350325945085, 0.9999994561850443, 0.9979024174199886, 0.9803526425471801, 0.004756675038922593, 0.0036419457635192645, 0.9994416345559775, 0.0069241994538546785, 0.0036359646898261344, 0.003663441844471378, 0.004004016819068384, 0.003821894704665985, 0.003882559955865276, 0.9977501659211743, 0.0037786575137754496, 0.003640093621657202, 0.003724323233934631, 0.9992329317642419, 0.003752592637956618, 0.007094495736264622, 0.9988901116280191, 0.997495786716563, 0.9993603401185973, 0.004373394593866966, 0.003733993990336617, 0.998790156689649, 0.9983297586699718, 0.005762688778576341, 0.9995785300850893, 0.9999968059355038, 0.015360815590208066, 0.9991868220138308, 0.004701381219956796, 0.004344856181592564, 0.003681039527820893, 0.9997534335254206, 0.0047630991118785765, 0.0053236354282785265, 0.04292764374713171, 0.003721076861209829, 0.003885147160253474, 0.003753500319053724, 0.9996470480833057, 0.9984333512697542, 0.0038867254686013905, 0.0060049923202926386, 0.003637305026734371, 0.003682258659661159, 0.003941765203114459, 0.003652587998287002, 0.00398415870069397, 0.9999876754139542, 0.9673857869043929, 0.004058778228050358, 0.023906856394037397, 0.0044385321171390825, 0.04292764374713171, 0.003978023417992294, 0.9995514922043759, 0.0055049695835094545, 0.997966255505733]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.99      0.82       141\n",
            "           1       0.97      0.55      0.70       134\n",
            "\n",
            "    accuracy                           0.77       275\n",
            "   macro avg       0.84      0.77      0.76       275\n",
            "weighted avg       0.83      0.77      0.76       275\n",
            "\n",
            "learning rate: 0.049999999999999996\n",
            "test set\n",
            "[[ -1.8391   -9.0883    9.2416   -0.10432]\n",
            " [ -3.6961  -13.6779   17.5795   -2.6181 ]\n",
            " [ -4.9462    3.5716    0.82742  -1.4957 ]\n",
            " ...\n",
            " [  0.47368   3.3605   -4.5064   -4.0431 ]\n",
            " [ -1.2576    1.5892    7.0078    0.42455]\n",
            " [  0.59823   3.5012   -3.9795   -1.7841 ]]\n",
            "label\n",
            "[1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 0 1\n",
            " 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1\n",
            " 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0\n",
            " 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1]\n",
            "prediction\n",
            "[0.9973820570662387, 0.9932235352674117, 0.9999606727293942, 0.9997364747366876, 0.9983910420886525, 0.9999997889299713, 0.9893228702107643, 0.003224748842716259, 0.002905311039428284, 0.0032124141609123718, 0.9999888514739332, 0.9997640058025657, 0.9995365003838185, 0.9999790303994314, 0.003049785998537245, 0.997324631688856, 0.0028955892338146045, 0.9999787248979431, 0.002904199437524966, 0.008416001315586188, 0.00290353556831937, 0.00728059951260547, 0.018934450226670292, 0.0035265965960752854, 0.9970512980648298, 0.9551326923498178, 0.003312827459548778, 0.9946690111632537, 0.9970535516006234, 0.0030090890261809554, 0.997683061061372, 0.9999581719157241, 0.9989185750654883, 0.0031609082867950964, 0.0028900347896546326, 0.9883250044255197, 0.0028819720169180954, 0.006697717073328076, 0.002894385291654586, 0.003295979688917853, 0.0030753948389898637, 0.9970951870151078, 0.003621184738008559, 0.011309092957014957, 0.9959732211066105, 0.9963904204776356, 0.003046565575029883, 0.007862163245689361, 0.998202659794743, 0.9966480048414545, 0.003991718446538057, 0.003029773422369603, 0.002893416195071816, 0.999874051665195, 0.003174469284864839, 0.9998605591000316, 0.999791596725393, 0.0030526558813299036, 0.9975209875235773, 0.9999872087564097, 0.9990862473041338, 0.04509572436820508, 0.9986580932283039, 0.9999994625682374, 0.036166588975343576, 0.003610405213173752, 0.9995888642500025, 0.9999988693432506, 0.9930056310687111, 0.005649770884270868, 0.9992461480445392, 0.003106295616282521, 0.003064828683517082, 0.9999816871670308, 0.9892396693931551, 0.0029704807428034253, 0.003111709706757067, 0.004687749420171049, 0.9999212126300897, 0.0030286153649894524, 0.9995232683438957, 0.0030310005998012736, 0.006001180098696098, 0.0029210091553056733, 0.00306837171414907, 0.9975531427993596, 0.00292989183337647, 0.003218132738950195, 0.0036065411344844618, 0.9982546885895155, 0.9848586485104571, 0.9999998035526159, 0.9980725738325137, 0.999240136982331, 0.9981236552742939, 0.9999871115402238, 0.9987332794776519, 0.9814018219182108, 0.9976393725923198, 0.9869029992568317, 0.9998814948748174, 0.9999890034272252, 0.9985303418752426, 0.9999993818506627, 0.9976774961923197, 0.004069497516567974, 0.9999237318207632, 0.003487857357671915, 0.9998620303431269, 0.0029473037038151607, 0.9999994717190157, 0.9999617579238045, 0.9984980798934713, 0.003467372446864858, 0.9981317978324845, 0.003142716401379393, 0.006084812670858256, 0.00848616846730499, 0.99998603829244, 0.998064337284499, 0.9984161546706255, 0.0029574836386050095, 0.003134199718878296, 0.0029365654264648516, 0.003558063188917989, 0.0031521774792041255, 0.0081510947796272, 0.999854440047247, 0.0030042713147549064, 0.9943135698537942, 0.9945014944439943, 0.0031325769244329492, 0.998280514069632, 0.0030847334660218887, 0.007335566207338936, 0.9999945503823733, 0.031099456801010117, 0.004392518885753377, 0.0028965537794228735, 0.9987786395808094, 0.002902503049499185, 0.9955090955047695, 0.0045261497211048, 0.9916722608616482, 0.9990698201728196, 0.9999858982755975, 0.0032394449231544975, 0.0028848209517046463, 0.002882320851414427, 0.003895395706641131, 0.0029118236934175933, 0.0029024168358024014, 0.9512511180710138, 0.002927350019993494, 0.026642261948157817, 0.005410918516767389, 0.9994653531262322, 0.0029647725951947182, 0.004019365206501421, 0.9999959634071537, 0.0029288550632341633, 0.9996410745670111, 0.998209396238348, 0.002901074921485305, 0.0032089230273325982, 0.9893857354732618, 0.9983852113483993, 0.9998359285319878, 0.9960693128063244, 0.9999435934555262, 0.999937250738722, 0.003019385471795691, 0.9999984254863363, 0.9998572001823153, 0.9940161647538329, 0.00392185961026995, 0.9991083823675673, 0.9999405844648374, 0.0028864718827773302, 0.0029958657158010225, 0.004833466976617021, 0.9999579933620941, 0.9996325727093576, 0.9999923868633265, 0.9996152080369679, 0.005188183795976305, 0.002997890172691275, 0.998585334342271, 0.9997328746074299, 0.9999801666404766, 0.0029044663657345746, 0.9999703439359331, 0.0029051527930706333, 0.9982653626576566, 0.0029038059667852454, 0.0031868478458079727, 0.9731042224934227, 0.9937135349263927, 0.9976102270858443, 0.0029318342088573792, 0.9997981348522323, 0.002916694534739076, 0.9999993155495464, 0.0028935790402753836, 0.003135908625755868, 0.003627760846081113, 0.999999511606372, 0.0037842253006688943, 0.0032910403824015977, 0.0035917921227529943, 0.999917157719921, 0.9993800660811937, 0.9999501951493243, 0.9999887931571173, 0.0034218378470874194, 0.9999997968617842, 0.9971244628207784, 0.9830232640748915, 0.003268532160637598, 0.002883851453178511, 0.9996223935654127, 0.007659788829892552, 0.002899925678081377, 0.002905257008031416, 0.0028982761536801827, 0.003051511426112265, 0.0031875725636925506, 0.9972430511860624, 0.0031859236753456087, 0.0028829635984399813, 0.002958169356229129, 0.9998332397260241, 0.0029819238650873854, 0.007685922280130529, 0.9981875885857762, 0.9997264317710033, 0.9996912804630248, 0.003167382803297376, 0.003043382313456874, 0.9980854719805342, 0.9978885282302126, 0.005029627058958907, 0.9998788557500714, 0.9999994053325335, 0.00624120079133105, 0.999284596156987, 0.0034515404834196406, 0.003508826792378387, 0.0029463027329958602, 0.9998544615660195, 0.004431725646646987, 0.003810080961243252, 0.0313028406412659, 0.0029195657149255058, 0.00289078497594918, 0.0031921792352866893, 0.9999177924187511, 0.9977425879486939, 0.0029949645035563114, 0.00499273776519946, 0.0028988370089048847, 0.0028888175464299973, 0.003046447945996863, 0.0029144695087888195, 0.0028907496233273977, 0.9999924610793104, 0.9807604599930422, 0.003425693430621275, 0.039308355871852704, 0.0035905332842920364, 0.0313028406412659, 0.003409330536777178, 0.9993278202435855, 0.004266132248555747, 0.9985232572060878]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.89      0.79       141\n",
            "           1       0.84      0.61      0.71       134\n",
            "\n",
            "    accuracy                           0.75       275\n",
            "   macro avg       0.77      0.75      0.75       275\n",
            "weighted avg       0.77      0.75      0.75       275\n",
            "\n",
            "learning rate: 0.06999999999999999\n",
            "test set\n",
            "[[ -1.8391   -9.0883    9.2416   -0.10432]\n",
            " [ -3.6961  -13.6779   17.5795   -2.6181 ]\n",
            " [ -4.9462    3.5716    0.82742  -1.4957 ]\n",
            " ...\n",
            " [  0.47368   3.3605   -4.5064   -4.0431 ]\n",
            " [ -1.2576    1.5892    7.0078    0.42455]\n",
            " [  0.59823   3.5012   -3.9795   -1.7841 ]]\n",
            "label\n",
            "[1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 0 1\n",
            " 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1\n",
            " 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0\n",
            " 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1]\n",
            "prediction\n",
            "[0.9989936399864056, 0.9946560409482749, 0.9998345629115668, 0.9987011289791571, 0.9992549218116853, 0.9999998996477039, 0.9912289364661926, 0.0026902023324363155, 0.0026411163994763034, 0.002726553871225579, 0.9999940114914248, 0.9996298266434395, 0.9996156657426631, 0.9999959493449287, 0.00267234444782925, 0.9989087539074467, 0.0026324205053686453, 0.9999849207183508, 0.00264135240662521, 0.006149140035260329, 0.002649102470622026, 0.004841713228777024, 0.019877737866564594, 0.0030841919967668275, 0.9984956234326746, 0.9788600643975386, 0.002752063724534882, 0.9960524167798495, 0.9989826912795842, 0.002663977503508678, 0.9994334251370364, 0.9999883799299318, 0.998322000026301, 0.002756533036726758, 0.0026349560469555943, 0.9933998466043129, 0.00263205810758389, 0.00722295234034893, 0.002635720498897737, 0.0027978682143906326, 0.002669454103541695, 0.998739613726115, 0.0031728897837234103, 0.006506638613799771, 0.9981166936409532, 0.9983573370944261, 0.002753637449674318, 0.003687699555418949, 0.999373517147418, 0.9986175340671697, 0.0030064472668515085, 0.002659096908124205, 0.0026389998243136984, 0.9999578213780757, 0.0028611774125969353, 0.9999528471590639, 0.9994391504762616, 0.0027172941663470857, 0.9988769082502118, 0.9999973866271316, 0.9996374648734714, 0.0730461122797039, 0.9994415293188754, 0.9999990233396512, 0.033239814111953404, 0.003305901061375576, 0.9997259402471721, 0.9999993338483532, 0.9907918853495833, 0.004346644336265696, 0.9996637769823299, 0.0026969737555101628, 0.002685295970140232, 0.9999966209724159, 0.9899717237560159, 0.0026961373620663876, 0.0027646441484335325, 0.0052691033766209755, 0.9999499536067464, 0.002662675439379451, 0.9998062715098597, 0.0026597605240081396, 0.003531640908005266, 0.002679456119246806, 0.002765876110734317, 0.9953828130874598, 0.0027082780044877246, 0.0027593741443494237, 0.0034100210088195593, 0.9994254010129215, 0.9904802843044855, 0.9999999117974918, 0.9965245215574808, 0.9996714260577391, 0.9993383827919687, 0.9999913607958375, 0.9993663199180844, 0.9844709525479554, 0.9990063961744243, 0.9888433207181171, 0.9998000133518399, 0.999997949831623, 0.9995243485672456, 0.9999996356140624, 0.9993110416538674, 0.003018858888209625, 0.9998534731102011, 0.0028054232017112692, 0.9999053765303227, 0.0026418063083746587, 0.9999990288415327, 0.9999917722276533, 0.9985482305426581, 0.003069862452295149, 0.9993618952246677, 0.002702711380161072, 0.006159806881869493, 0.005307770208461973, 0.9999968787837081, 0.9992564982901333, 0.999419838516389, 0.002647186332567723, 0.0027502577443674616, 0.0026421073261976586, 0.0035086629529620206, 0.0027058203004014453, 0.004970092650564567, 0.9999523247136317, 0.002679972769153294, 0.9958135612806087, 0.9980043376252592, 0.0027011780621275295, 0.9994574593868489, 0.002791985192768528, 0.004290524744852977, 0.9999523797198822, 0.02551419610311042, 0.003717133623831903, 0.002638970986882992, 0.9983527709469093, 0.002636691861951184, 0.997740301461301, 0.0030707658481597855, 0.9928562327582547, 0.9969657049394777, 0.9999120977023842, 0.002923275048296667, 0.002634697479773035, 0.002630914604328937, 0.00313011223820479, 0.002636643250728627, 0.0026451418017929846, 0.9770271729604808, 0.0026717254759030334, 0.006528768787877325, 0.003717584269021329, 0.9995687689720193, 0.0026989451202108555, 0.002975424345954226, 0.9999824196803621, 0.0026422863582953216, 0.9997509613004232, 0.9994400733596213, 0.002642732779340852, 0.0027032887041145894, 0.9863647432416112, 0.9995262001194472, 0.999944711309387, 0.9974882595666789, 0.9998804717301256, 0.9998706673386507, 0.0026545387021329227, 0.9999972676518637, 0.9999051918626896, 0.9936752395344051, 0.0031099640632815838, 0.9995515648342088, 0.9999863894944202, 0.002632908867756394, 0.0026652812874869212, 0.0036284982098160213, 0.9999907748986496, 0.9998124050625402, 0.9999955284607583, 0.9997430696275719, 0.004201363721718704, 0.002692014147234344, 0.9994673352518245, 0.9989058091386022, 0.9999962016368917, 0.0026549935939631612, 0.9999772897954803, 0.0026627024428759324, 0.9995227050013727, 0.002636442907394037, 0.0027589702260158704, 0.9699642996772023, 0.9958187717982588, 0.9963440936384155, 0.0026475058455541827, 0.9999240213729401, 0.0026383376810640343, 0.9999985671453928, 0.0026371703368680043, 0.0029262233743730403, 0.0033020624722515913, 0.9999998373012311, 0.003085098892119252, 0.002769698142881348, 0.003166926514313662, 0.9999760611325654, 0.9993853251944765, 0.9998660245910965, 0.9999978312633181, 0.0034218678780534445, 0.9999999065659698, 0.9988231338336052, 0.9835984557528712, 0.0030985102263252325, 0.0026318375450348304, 0.9998152043321178, 0.00575456494735268, 0.0026386705997782393, 0.002634090393298889, 0.0026669028865542468, 0.002753282756999068, 0.0027630392030893462, 0.9986038239189737, 0.0027264611747557866, 0.002631275508640187, 0.0026544803976087552, 0.9996759179211813, 0.0026597894302216813, 0.005759324205908461, 0.9994127999856035, 0.9988365933525173, 0.999751767541426, 0.0028463329271008115, 0.0026908325058122, 0.9994223089877677, 0.9989417476929531, 0.0031454882517363675, 0.9998429123980909, 0.9999988237310258, 0.006787456954074413, 0.9996829831590605, 0.002972200531550822, 0.0028308936467619597, 0.002644117191075583, 0.9999230801617794, 0.0036680861265215615, 0.003391270070206592, 0.032572437216710784, 0.0026402831330572863, 0.0026522650716761854, 0.002712520563690383, 0.9998538399751133, 0.9990206825542233, 0.0026889773892719284, 0.0033813155328317086, 0.0026412806234256975, 0.002637678798484371, 0.002745783595379914, 0.0026500274201295554, 0.0026604379496743737, 0.9999979893470692, 0.9839158915873127, 0.002829243661586759, 0.008162335399029153, 0.003022860799804663, 0.032572437216710784, 0.00286092809395919, 0.9997006202223047, 0.0033966689408697513, 0.9990188207768415]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.06      0.04      0.05       141\n",
            "           1       0.20      0.25      0.22       134\n",
            "\n",
            "    accuracy                           0.15       275\n",
            "   macro avg       0.13      0.15      0.14       275\n",
            "weighted avg       0.13      0.15      0.13       275\n",
            "\n",
            "learning rate: 0.08999999999999998\n",
            "test set\n",
            "[[ -1.8391   -9.0883    9.2416   -0.10432]\n",
            " [ -3.6961  -13.6779   17.5795   -2.6181 ]\n",
            " [ -4.9462    3.5716    0.82742  -1.4957 ]\n",
            " ...\n",
            " [  0.47368   3.3605   -4.5064   -4.0431 ]\n",
            " [ -1.2576    1.5892    7.0078    0.42455]\n",
            " [  0.59823   3.5012   -3.9795   -1.7841 ]]\n",
            "label\n",
            "[1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1 0\n",
            " 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 1 0 0 1\n",
            " 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1\n",
            " 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0\n",
            " 0 0 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 1\n",
            " 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0\n",
            " 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0\n",
            " 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1]\n",
            "prediction\n",
            "[0.9981325447336121, 0.9963627920773669, 0.999677343604808, 0.9996605326450223, 0.9987217840237346, 0.9999999330708718, 0.9958396156510539, 0.0029366301469320234, 0.002825716778305301, 0.002956144826253945, 0.9999941958099698, 0.9999572680824451, 0.9992060001108598, 0.9999995684727477, 0.002921856415233765, 0.9983510586365691, 0.002823704482295655, 0.999986977855532, 0.0028368279388604855, 0.008478737683283818, 0.0028404572982159257, 0.006535151109813989, 0.030449013237944425, 0.0037798145740314147, 0.9987276683541207, 0.9937842169191066, 0.002871683569094269, 0.9966772822048928, 0.9956643328110226, 0.0028695181797625396, 0.9990111357809243, 0.9999986326894418, 0.9997757440855717, 0.0029802985707541255, 0.002826441745625249, 0.9948163832910544, 0.0028221648084929096, 0.012949339259654021, 0.002822191628516513, 0.003047145347028884, 0.002910111627575825, 0.9982147246458958, 0.00373595117544006, 0.004872231535233752, 0.9981356757959429, 0.9976677688981959, 0.0029698565264429646, 0.0031073861502573293, 0.998609329682573, 0.9981673866412891, 0.002835343657445872, 0.002889852502386418, 0.002831370355829408, 0.9999938527192511, 0.0030974839442577573, 0.9999925291844233, 0.9999360356182263, 0.002924434810000654, 0.9982878513460196, 0.9999998382153577, 0.9991320891088558, 0.2944634613659251, 0.9991310136287982, 0.9999990452356265, 0.03653802976075841, 0.004452830751229887, 0.9992335536909026, 0.9999994540073595, 0.9900836985659128, 0.006182513647730184, 0.9992090510371577, 0.0029412138763389444, 0.0029220728131480428, 0.9999996818789287, 0.9957866977218045, 0.002895601473003671, 0.002989080248908457, 0.006292453863670142, 0.9999357672052611, 0.002900830655361737, 0.9996543791602955, 0.002891425256127333, 0.0036808042551835906, 0.00286720475015024, 0.0029809861396082947, 0.9966560417088733, 0.0029223066610142945, 0.0030954140733812437, 0.004631512785195403, 0.9987345436421288, 0.9962927585173226, 0.9999999437941474, 0.9990999469101906, 0.9992337247791531, 0.9986301058196393, 0.9999967038906661, 0.9988333542446709, 0.9757046131025419, 0.9984923776584572, 0.9955653663242365, 0.9995655484739958, 0.9999998958463057, 0.9989038107533618, 0.9999997230963923, 0.9986939638148057, 0.0032890481037089834, 0.9997170623381733, 0.0030826732760784433, 0.9999434064161095, 0.0028468621514126037, 0.9999992230711646, 0.9999993039806278, 0.9983743536184463, 0.003896153299340534, 0.9987196127336833, 0.0029844672154008193, 0.006542320630234821, 0.003291712349855429, 0.9999995489727387, 0.9985403860421596, 0.9989252162478698, 0.0028444251572600976, 0.002982742316941722, 0.0028350477728459047, 0.003796992786080783, 0.002976400452441288, 0.0058141323432057785, 0.9999734778452964, 0.0029151228252315376, 0.9962696499023251, 0.9983576140559323, 0.0029438495986935397, 0.9986944620220571, 0.0029999541929501923, 0.0029740003893983557, 0.9999016992595607, 0.02498164794325138, 0.004514423581938289, 0.002832058628368354, 0.9995938984576241, 0.002826853278485056, 0.997118549738279, 0.003209706346064942, 0.991694357450884, 0.9990261062017928, 0.9997929470924346, 0.0031633174901038023, 0.0028266467964931956, 0.002817532497643057, 0.0035581011459615416, 0.0028268239086219494, 0.002840936873974408, 0.9934692559148789, 0.0028767153296204765, 0.003070003281699064, 0.004120109979916496, 0.9998109889118356, 0.002919925343836348, 0.003224196616325232, 0.9999822249751072, 0.0028352694073427906, 0.9995875931861944, 0.9987337942325711, 0.002833421991846951, 0.0029630053464991144, 0.9928045882046471, 0.9987925780934926, 0.9999712508419366, 0.9974708448936571, 0.9998147198489712, 0.9998153749839871, 0.0028793072484862813, 0.9999982487999892, 0.9999483124120454, 0.9987747115564821, 0.003393670956553443, 0.9996611424446459, 0.9999968408257207, 0.0028197768770917286, 0.002869970862088962, 0.0030812355690523667, 0.9999991673666523, 0.9999557541035609, 0.999996671709656, 0.9993593734769115, 0.005621157559105534, 0.002895651252223437, 0.9989256120812345, 0.999813507905079, 0.9999997744511799, 0.002850863382698847, 0.9999882119109428, 0.0028684329544950544, 0.9988950543915862, 0.00282767512506969, 0.0030884457720454987, 0.9468079813901058, 0.9944587717252582, 0.9990677544805404, 0.0028436245707624975, 0.9999158362647754, 0.002830320721345906, 0.9999986870593257, 0.002825932148722483, 0.003145879029219819, 0.004443946128511587, 0.999999933062508, 0.0031640311592136838, 0.003010223475033326, 0.0037696657481630954, 0.9999927804067686, 0.9988604700196901, 0.9996930454076224, 0.9999998905387277, 0.0036132325995835144, 0.9999999386349487, 0.9980495175799656, 0.9789407898034254, 0.003328449076921224, 0.002818777633721489, 0.9999568629074964, 0.007233991091040475, 0.002827998029844098, 0.002825746589193268, 0.0028751285673302577, 0.0029604361123676606, 0.0030697760262182617, 0.997701845353508, 0.0029645242605753856, 0.002817796883987427, 0.002855774430538279, 0.99996657069786, 0.0028645539800468147, 0.008973905055158207, 0.9986767285440729, 0.99978893190246, 0.9999619598235504, 0.0030816102524178473, 0.002963541131758275, 0.9986779514420339, 0.9982356310810061, 0.0029698609467715435, 0.9997593462404536, 0.9999988656849289, 0.004901010056919374, 0.9991686964201744, 0.0032151921561169063, 0.0028271142818282234, 0.002850149266346375, 0.9999790469164126, 0.005088114509116974, 0.0037031030804546886, 0.04052369027311573, 0.002829111129799525, 0.002856544129709792, 0.00294185740118287, 0.999843702399621, 0.9984751465865322, 0.0028926793182679623, 0.002852385819814581, 0.002837227514869838, 0.0028302382116891845, 0.002972559034706703, 0.0028454841619817857, 0.0028691674450751774, 0.9999996524750143, 0.9929436867993794, 0.0031091802721229726, 0.003209163118710464, 0.0033503122364263492, 0.04052369027311573, 0.003276012419725651, 0.9995226281271681, 0.0031600025248609905, 0.9994465577616525]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.91      0.72       141\n",
            "           1       0.79      0.33      0.46       134\n",
            "\n",
            "    accuracy                           0.63       275\n",
            "   macro avg       0.69      0.62      0.59       275\n",
            "weighted avg       0.68      0.63      0.59       275\n",
            "\n"
          ]
        }
      ]
    }
  ]
}